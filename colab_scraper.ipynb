{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# AGN Health Q&A Scraper - Google Colab\n",
    "\n",
    "Notebook ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scrape ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Q&A ‡∏à‡∏≤‡∏Å AGN Health Forums ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á MongoDB Atlas\n",
    "\n",
    "## ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:\n",
    "1. ‡∏£‡∏±‡∏ô Cell ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies\n",
    "2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ environment variables\n",
    "3. ‡∏£‡∏±‡∏ô scraper\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "## 1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Chrome ‡πÅ‡∏•‡∏∞ ChromeDriver\n",
    "!apt-get update\n",
    "!apt-get install -y chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "\n",
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Python packages\n",
    "!pip install selenium beautifulsoup4 pymongo webdriver-manager python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 2. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-vars"
   },
   "outputs": [],
   "source": [
    "# MongoDB Configuration\n",
    "MONGODB_URL = \"mongodb+srv://natthapiw_db_user:afOJe2MrgMDsmm6k@cluster0.skadipr.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "MONGODB_DATABASE = \"agn\"\n",
    "MONGODB_COLLECTION = \"qa\"\n",
    "\n",
    "# Scraper Configuration\n",
    "SCRAPER_START_ID = 1\n",
    "SCRAPER_END_ID = 2675\n",
    "SCRAPER_MIN_DELAY = 2\n",
    "SCRAPER_MAX_DELAY = 5\n",
    "\n",
    "BASE_URL = \"https://www.agnoshealth.com/forums\"\n",
    "\n",
    "print(\"‚úÖ Configuration set successfully!\")\n",
    "print(f\"üìä Will scrape threads {SCRAPER_START_ID} to {SCRAPER_END_ID}\")\n",
    "print(f\"üóÑÔ∏è  Database: {MONGODB_DATABASE}.{MONGODB_COLLECTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scraper-code"
   },
   "source": [
    "## 3. Scraper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scraper-class"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import random\n",
    "from typing import Optional, Dict\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class AGNHealthScraper:\n",
    "    \"\"\"Scraper for AGN Health Q&A forums.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the scraper with MongoDB connection and Selenium driver.\"\"\"\n",
    "        self.mongo_client = None\n",
    "        self.db = None\n",
    "        self.collection = None\n",
    "        self.driver = None\n",
    "        self._setup_mongodb()\n",
    "        self._setup_selenium()\n",
    "\n",
    "    def _setup_mongodb(self):\n",
    "        \"\"\"Set up MongoDB connection and ensure indexes.\"\"\"\n",
    "        try:\n",
    "            self.mongo_client = MongoClient(MONGODB_URL)\n",
    "            self.db = self.mongo_client[MONGODB_DATABASE]\n",
    "            self.collection = self.db[MONGODB_COLLECTION]\n",
    "\n",
    "            # Create unique index on thread_id to prevent duplicates\n",
    "            self.collection.create_index(\"thread_id\", unique=True)\n",
    "            logger.info(\"MongoDB connection established successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to MongoDB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _setup_selenium(self):\n",
    "        \"\"\"Set up Selenium WebDriver with Chrome.\"\"\"\n",
    "        try:\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument('--headless')\n",
    "            chrome_options.add_argument('--no-sandbox')\n",
    "            chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "            chrome_options.add_argument('--disable-gpu')\n",
    "            chrome_options.add_argument('--window-size=1920,1080')\n",
    "            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            logger.info(\"Selenium WebDriver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Selenium: {e}\")\n",
    "            raise\n",
    "\n",
    "    def scrape_thread(self, thread_id: int) -> Optional[Dict]:\n",
    "        \"\"\"Scrape a single Q&A thread.\"\"\"\n",
    "        url = f\"{BASE_URL}/{thread_id}\"\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"Scraping thread {thread_id}...\")\n",
    "            self.driver.get(url)\n",
    "\n",
    "            # Wait for page to load\n",
    "            wait = WebDriverWait(self.driver, 10)\n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"main\")))\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "\n",
    "            # Extract data\n",
    "            data = {\n",
    "                'thread_id': thread_id,\n",
    "                'date': self._extract_date(soup),\n",
    "                'topic': self._extract_topic(soup),\n",
    "                'question': self._extract_question(soup),\n",
    "                'answer': self._extract_answer(soup)\n",
    "            }\n",
    "\n",
    "            # Validate that we have at least question or topic\n",
    "            if not data['question'] and not data['topic']:\n",
    "                logger.warning(f\"Thread {thread_id}: No valid content found\")\n",
    "                return None\n",
    "\n",
    "            logger.info(f\"Thread {thread_id}: Successfully scraped\")\n",
    "            return data\n",
    "\n",
    "        except TimeoutException:\n",
    "            logger.warning(f\"Thread {thread_id}: Timeout - page may not exist\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Thread {thread_id}: Error during scraping - {e}\")\n",
    "            return None\n",
    "\n",
    "    def _extract_date(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract date from the page.\"\"\"\n",
    "        try:\n",
    "            date_elem = soup.select_one('time span.text-sm.text-gray-500')\n",
    "            if date_elem:\n",
    "                return date_elem.get_text(strip=True)\n",
    "            time_elem = soup.select_one('time')\n",
    "            if time_elem:\n",
    "                return time_elem.get_text(strip=True)\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Date extraction error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _extract_topic(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract topic from the page.\"\"\"\n",
    "        try:\n",
    "            topic_elem = soup.select_one('article p.font-bold')\n",
    "            if topic_elem:\n",
    "                return topic_elem.get_text(strip=True)\n",
    "            topic_elem = soup.select_one('article div.flex-col p')\n",
    "            if topic_elem:\n",
    "                return topic_elem.get_text(strip=True)\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Topic extraction error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _extract_question(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract question from the page.\"\"\"\n",
    "        try:\n",
    "            question_elem = soup.select_one('span.font-bold.text-lg')\n",
    "            if question_elem:\n",
    "                return question_elem.get_text(strip=True)\n",
    "            question_div = soup.select_one('div.rounded-2xl.border.border-blue-100 span')\n",
    "            if question_div:\n",
    "                return question_div.get_text(strip=True)\n",
    "            section = soup.select_one('section.space-y-4 span.font-bold')\n",
    "            if section:\n",
    "                return section.get_text(strip=True)\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Question extraction error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _extract_answer(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract answer from the page.\"\"\"\n",
    "        try:\n",
    "            answer_parts = []\n",
    "            li_elements = soup.select('section.space-y-4 ul li p')\n",
    "            if li_elements:\n",
    "                for elem in li_elements:\n",
    "                    text = elem.get_text(strip=True)\n",
    "                    if text:\n",
    "                        answer_parts.append(text)\n",
    "\n",
    "            if not answer_parts:\n",
    "                p_elements = soup.select('section.space-y-4 p.mt-4')\n",
    "                for elem in p_elements:\n",
    "                    text = elem.get_text(strip=True)\n",
    "                    if text:\n",
    "                        answer_parts.append(text)\n",
    "\n",
    "            if not answer_parts:\n",
    "                p_elements = soup.select('section ul li p, section p')\n",
    "                for elem in p_elements:\n",
    "                    text = elem.get_text(strip=True)\n",
    "                    if text and len(text) > 20:\n",
    "                        answer_parts.append(text)\n",
    "\n",
    "            return \"\\n\\n\".join(answer_parts) if answer_parts else \"\"\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Answer extraction error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def save_to_mongodb(self, data: Dict) -> bool:\n",
    "        \"\"\"Save scraped data to MongoDB.\"\"\"\n",
    "        try:\n",
    "            self.collection.insert_one(data)\n",
    "            logger.info(f\"Thread {data['thread_id']}: Saved to MongoDB\")\n",
    "            return True\n",
    "        except DuplicateKeyError:\n",
    "            logger.info(f\"Thread {data['thread_id']}: Already exists, skipping\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Thread {data['thread_id']}: Failed to save - {e}\")\n",
    "            return False\n",
    "\n",
    "    def scrape_all(self, start_id: int, end_id: int):\n",
    "        \"\"\"Scrape all threads in the specified range.\"\"\"\n",
    "        logger.info(f\"Starting scraper for threads {start_id} to {end_id}\")\n",
    "\n",
    "        success_count = 0\n",
    "        skip_count = 0\n",
    "        error_count = 0\n",
    "\n",
    "        for thread_id in range(start_id, end_id + 1):\n",
    "            try:\n",
    "                # Scrape the thread\n",
    "                data = self.scrape_thread(thread_id)\n",
    "\n",
    "                if data:\n",
    "                    if self.save_to_mongodb(data):\n",
    "                        success_count += 1\n",
    "                    else:\n",
    "                        skip_count += 1\n",
    "                else:\n",
    "                    error_count += 1\n",
    "\n",
    "                # Add random delay\n",
    "                delay = random.uniform(SCRAPER_MIN_DELAY, SCRAPER_MAX_DELAY)\n",
    "                time.sleep(delay)\n",
    "\n",
    "                # Log progress every 50 threads\n",
    "                if thread_id % 50 == 0:\n",
    "                    logger.info(f\"Progress: {thread_id}/{end_id} - Success: {success_count}, Skipped: {skip_count}, Errors: {error_count}\")\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                logger.info(\"Scraping interrupted by user\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error processing thread {thread_id}: {e}\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"Scraping completed! Success: {success_count}, Skipped: {skip_count}, Errors: {error_count}\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logger.info(\"Selenium driver closed\")\n",
    "        if self.mongo_client:\n",
    "            self.mongo_client.close()\n",
    "            logger.info(\"MongoDB connection closed\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Scraper class loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run"
   },
   "source": [
    "## 4. ‡∏£‡∏±‡∏ô Scraper\n",
    "\n",
    "‚ö†Ô∏è **‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô**: ‡∏Å‡∏≤‡∏£ scrape 2,675 threads ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 2-4 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:**\n",
    "- ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡πâ `end_id` ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ô‡πâ‡∏≠‡∏¢‡πÜ ‡πÄ‡∏ä‡πà‡∏ô 100\n",
    "- ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ scrape ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÉ‡∏ä‡πâ `SCRAPER_END_ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-scraper"
   },
   "outputs": [],
   "source": [
    "# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏≤‡∏á threads\n",
    "start_id = SCRAPER_START_ID  # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà 1\n",
    "end_id = 2764  \n",
    "\n",
    "scraper = None\n",
    "try:\n",
    "    print(\"üöÄ Starting scraper...\")\n",
    "    print(f\"üìä Scraping threads {start_id} to {end_id}\")\n",
    "    print(\"‚è±Ô∏è  This may take a while...\\n\")\n",
    "    \n",
    "    scraper = AGNHealthScraper()\n",
    "    scraper.scrape_all(start_id, end_id)\n",
    "    \n",
    "    print(\"\\n‚úÖ Scraping completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "finally:\n",
    "    if scraper:\n",
    "        scraper.close()\n",
    "        print(\"üîí Resources cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify"
   },
   "source": [
    "## 5. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-data"
   },
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(MONGODB_URL)\n",
    "db = client[MONGODB_DATABASE]\n",
    "collection = db[MONGODB_COLLECTION]\n",
    "\n",
    "total_docs = collection.count_documents({})\n",
    "print(f\"üìä Total documents in database: {total_docs}\")\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "if total_docs > 0:\n",
    "    print(\"\\nüìÑ Sample document:\")\n",
    "    sample = collection.find_one()\n",
    "    if sample:\n",
    "        print(f\"  Thread ID: {sample.get('thread_id')}\")\n",
    "        print(f\"  Date: {sample.get('date')}\")\n",
    "        print(f\"  Topic: {sample.get('topic')[:50]}...\" if sample.get('topic') else \"  Topic: N/A\")\n",
    "        print(f\"  Question: {sample.get('question')[:50]}...\" if sample.get('question') else \"  Question: N/A\")\n",
    "        print(f\"  Answer length: {len(sample.get('answer', ''))} characters\")\n",
    "\n",
    "client.close()\n",
    "print(\"\\n‚úÖ Verification completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes"
   },
   "source": [
    "## üìù ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏\n",
    "\n",
    "### ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß:\n",
    "1. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô MongoDB Atlas\n",
    "2. ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô (‡∏°‡∏µ unique index ‡∏ö‡∏ô thread_id)\n",
    "3. ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏¢‡∏∏‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ (‡∏à‡∏∞ skip threads ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)\n",
    "\n",
    "### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ:\n",
    "- ‡∏£‡∏±‡∏ô `colab_embedder.ipynb` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings\n",
    "\n",
    "### Tips:\n",
    "- ‡∏ñ‡πâ‡∏≤ Colab timeout ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà ‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ\n",
    "- ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ scrape ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô ‡∏•‡∏î delay ‡πÉ‡∏ô configuration\n",
    "- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö logs ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
