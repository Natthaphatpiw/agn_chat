{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# AGN Health Q&A Scraper - Google Colab\n",
    "\n",
    "Notebook สำหรับ scrape ข้อมูล Q&A จาก AGN Health Forums และบันทึกลง MongoDB Atlas\n",
    "\n",
    "## ขั้นตอนการใช้งาน:\n",
    "1. รัน Cell ติดตั้ง dependencies\n",
    "2. ตั้งค่า environment variables\n",
    "3. รัน scraper\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "## 1. ติดตั้ง Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# ติดตั้ง Chrome และ ChromeDriver\n",
    "!apt-get update\n",
    "!apt-get install -y chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "\n",
    "# ติดตั้ง Python packages\n",
    "!pip install selenium beautifulsoup4 pymongo webdriver-manager python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 2. กำหนดค่า Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config-vars"
   },
   "outputs": [],
   "source": [
    "# MongoDB Configuration\n",
    "MONGODB_URL = \"mongodb+srv://natthapiw_db_user:afOJe2MrgMDsmm6k@cluster0.skadipr.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "MONGODB_DATABASE = \"agn\"\n",
    "MONGODB_COLLECTION = \"qa\"\n",
    "\n",
    "# Scraper Configuration\n",
    "SCRAPER_START_ID = 1\n",
    "SCRAPER_END_ID = 2675\n",
    "SCRAPER_MIN_DELAY = 2\n",
    "SCRAPER_MAX_DELAY = 5\n",
    "\n",
    "BASE_URL = \"https://www.agnoshealth.com/forums\"\n",
    "\n",
    "print(\"✅ Configuration set successfully!\")\n",
    "print(f\"📊 Will scrape threads {SCRAPER_START_ID} to {SCRAPER_END_ID}\")\n",
    "print(f\"🗄️  Database: {MONGODB_DATABASE}.{MONGODB_COLLECTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scraper-code"
   },
   "source": [
    "## 3. Scraper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scraper-class"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import random\n",
    "from typing import Optional, Dict\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class AGNHealthScraper:\n",
    "    \"\"\"Scraper for AGN Health Q&A forums.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the scraper with MongoDB connection and Selenium driver.\"\"\"\n",
    "        self.mongo_client = None\n",
    "        self.db = None\n",
    "        self.collection = None\n",
    "        self.driver = None\n",
    "        self._setup_mongodb()\n",
    "        self._setup_selenium()\n",
    "\n",
    "    def _setup_mongodb(self):\n",
    "        \"\"\"Set up MongoDB connection and ensure indexes.\"\"\"\n",
    "        try:\n",
    "            self.mongo_client = MongoClient(MONGODB_URL)\n",
    "            self.db = self.mongo_client[MONGODB_DATABASE]\n",
    "            self.collection = self.db[MONGODB_COLLECTION]\n",
    "\n",
    "            # Create unique index on thread_id to prevent duplicates\n",
    "            self.collection.create_index(\"thread_id\", unique=True)\n",
    "            logger.info(\"MongoDB connection established successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to MongoDB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _setup_selenium(self):\n",
    "        \"\"\"Set up Selenium WebDriver with Chrome.\"\"\"\n",
    "        try:\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument('--headless')\n",
    "            chrome_options.add_argument('--no-sandbox')\n",
    "            chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "            chrome_options.add_argument('--disable-gpu')\n",
    "            chrome_options.add_argument('--window-size=1920,1080')\n",
    "            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "\n",
    "            self.driver = webdriver.Chrome(options=chrome_options)\n",
    "            logger.info(\"Selenium WebDriver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize Selenium: {e}\")\n",
    "            raise\n",
    "\n",
    "    def scrape_thread(self, thread_id: int) -> Optional[Dict]:\n",
    "        \"\"\"Scrape a single Q&A thread.\"\"\"\n",
    "        url = f\"{BASE_URL}/{thread_id}\"\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"Scraping thread {thread_id}...\")\n",
    "            self.driver.get(url)\n",
    "\n",
    "            # Wait for page to load\n",
    "            wait = WebDriverWait(self.driver, 10)\n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"main\")))\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "\n",
    "            # Extract data\n",
    "            data = {\n",
    "                'thread_id': thread_id,\n",
    "                'date': self._extract_date(soup),\n",
    "                'topic': self._extract_topic(soup),\n",
    "                'question': self._extract_question(soup),\n",
    "                'answer': self._extract_answer(soup)\n",
    "            }\n",
    "\n",
    "            # Validate that we have at least question or topic\n",
    "            if not data['question'] and not data['topic']:\n",
    "                logger.warning(f\"Thread {thread_id}: No valid content found\")\n",
    "                return None\n",
    "\n",
    "            logger.info(f\"Thread {thread_id}: Successfully scraped\")\n",
    "            return data\n",
    "\n",
    "        except TimeoutException:\n",
    "            logger.warning(f\"Thread {thread_id}: Timeout - page may not exist\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Thread {thread_id}: Error during scraping - {e}\")\n",
    "            return None\n",
    "\n",
    "    def _extract_date(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract date from the page.\"\"\"\n",
    "        try:\n",
    "            date_elem = soup.select_one('time span.text-sm.text-gray-500')\n",
    "            if date_elem:\n",
    "                return date_elem.get_text(strip=True)\n",
    "            time_elem = soup.select_one('time')\n",
    "            if time_elem:\n",
    "                return time_elem.get_text(strip=True)\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Date extraction error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _extract_topic(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract topic from the page.\"\"\"\n",
    "        try:\n",
    "            topic_elem = soup.select_one('article p.font-bold')\n",
    "            if topic_elem:\n",
    "                return topic_elem.get_text(strip=True)\n",
    "            topic_elem = soup.select_one('article div.flex-col p')\n",
    "            if topic_elem:\n",
    "                return topic_elem.get_text(strip=True)\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Topic extraction error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _extract_question(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract question from the page.\"\"\"\n",
    "        try:\n",
    "            question_elem = soup.select_one('span.font-bold.text-lg')\n",
    "            if question_elem:\n",
    "                return question_elem.get_text(strip=True)\n",
    "            question_div = soup.select_one('div.rounded-2xl.border.border-blue-100 span')\n",
    "            if question_div:\n",
    "                return question_div.get_text(strip=True)\n",
    "            section = soup.select_one('section.space-y-4 span.font-bold')\n",
    "            if section:\n",
    "                return section.get_text(strip=True)\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Question extraction error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _extract_answer(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract answer from the page.\"\"\"\n",
    "        try:\n",
    "            answer_parts = []\n",
    "            li_elements = soup.select('section.space-y-4 ul li p')\n",
    "            if li_elements:\n",
    "                for elem in li_elements:\n",
    "                    text = elem.get_text(strip=True)\n",
    "                    if text:\n",
    "                        answer_parts.append(text)\n",
    "\n",
    "            if not answer_parts:\n",
    "                p_elements = soup.select('section.space-y-4 p.mt-4')\n",
    "                for elem in p_elements:\n",
    "                    text = elem.get_text(strip=True)\n",
    "                    if text:\n",
    "                        answer_parts.append(text)\n",
    "\n",
    "            if not answer_parts:\n",
    "                p_elements = soup.select('section ul li p, section p')\n",
    "                for elem in p_elements:\n",
    "                    text = elem.get_text(strip=True)\n",
    "                    if text and len(text) > 20:\n",
    "                        answer_parts.append(text)\n",
    "\n",
    "            return \"\\n\\n\".join(answer_parts) if answer_parts else \"\"\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Answer extraction error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def save_to_mongodb(self, data: Dict) -> bool:\n",
    "        \"\"\"Save scraped data to MongoDB.\"\"\"\n",
    "        try:\n",
    "            self.collection.insert_one(data)\n",
    "            logger.info(f\"Thread {data['thread_id']}: Saved to MongoDB\")\n",
    "            return True\n",
    "        except DuplicateKeyError:\n",
    "            logger.info(f\"Thread {data['thread_id']}: Already exists, skipping\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Thread {data['thread_id']}: Failed to save - {e}\")\n",
    "            return False\n",
    "\n",
    "    def scrape_all(self, start_id: int, end_id: int):\n",
    "        \"\"\"Scrape all threads in the specified range.\"\"\"\n",
    "        logger.info(f\"Starting scraper for threads {start_id} to {end_id}\")\n",
    "\n",
    "        success_count = 0\n",
    "        skip_count = 0\n",
    "        error_count = 0\n",
    "\n",
    "        for thread_id in range(start_id, end_id + 1):\n",
    "            try:\n",
    "                # Scrape the thread\n",
    "                data = self.scrape_thread(thread_id)\n",
    "\n",
    "                if data:\n",
    "                    if self.save_to_mongodb(data):\n",
    "                        success_count += 1\n",
    "                    else:\n",
    "                        skip_count += 1\n",
    "                else:\n",
    "                    error_count += 1\n",
    "\n",
    "                # Add random delay\n",
    "                delay = random.uniform(SCRAPER_MIN_DELAY, SCRAPER_MAX_DELAY)\n",
    "                time.sleep(delay)\n",
    "\n",
    "                # Log progress every 50 threads\n",
    "                if thread_id % 50 == 0:\n",
    "                    logger.info(f\"Progress: {thread_id}/{end_id} - Success: {success_count}, Skipped: {skip_count}, Errors: {error_count}\")\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                logger.info(\"Scraping interrupted by user\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error processing thread {thread_id}: {e}\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"Scraping completed! Success: {success_count}, Skipped: {skip_count}, Errors: {error_count}\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logger.info(\"Selenium driver closed\")\n",
    "        if self.mongo_client:\n",
    "            self.mongo_client.close()\n",
    "            logger.info(\"MongoDB connection closed\")\n",
    "\n",
    "\n",
    "print(\"✅ Scraper class loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run"
   },
   "source": [
    "## 4. รัน Scraper\n",
    "\n",
    "⚠️ **คำเตือน**: การ scrape 2,675 threads จะใช้เวลา 2-4 ชั่วโมง\n",
    "\n",
    "**ตัวเลือก:**\n",
    "- ถ้าต้องการทดสอบ ให้แก้ `end_id` เป็นจำนวนน้อยๆ เช่น 100\n",
    "- ถ้าต้องการ scrape ทั้งหมด ใช้ `SCRAPER_END_ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-scraper"
   },
   "outputs": [],
   "source": [
    "# เปลี่ยนตรงนี้ถ้าต้องการทดสอบเฉพาะบาง threads\n",
    "start_id = SCRAPER_START_ID  # เริ่มที่ 1\n",
    "end_id = 2764  \n",
    "\n",
    "scraper = None\n",
    "try:\n",
    "    print(\"🚀 Starting scraper...\")\n",
    "    print(f\"📊 Scraping threads {start_id} to {end_id}\")\n",
    "    print(\"⏱️  This may take a while...\\n\")\n",
    "    \n",
    "    scraper = AGNHealthScraper()\n",
    "    scraper.scrape_all(start_id, end_id)\n",
    "    \n",
    "    print(\"\\n✅ Scraping completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "finally:\n",
    "    if scraper:\n",
    "        scraper.close()\n",
    "        print(\"🔒 Resources cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify"
   },
   "source": [
    "## 5. ตรวจสอบผลลัพธ์"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-data"
   },
   "outputs": [],
   "source": [
    "# ตรวจสอบจำนวนข้อมูลที่บันทึกแล้ว\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(MONGODB_URL)\n",
    "db = client[MONGODB_DATABASE]\n",
    "collection = db[MONGODB_COLLECTION]\n",
    "\n",
    "total_docs = collection.count_documents({})\n",
    "print(f\"📊 Total documents in database: {total_docs}\")\n",
    "\n",
    "# แสดงตัวอย่างข้อมูล\n",
    "if total_docs > 0:\n",
    "    print(\"\\n📄 Sample document:\")\n",
    "    sample = collection.find_one()\n",
    "    if sample:\n",
    "        print(f\"  Thread ID: {sample.get('thread_id')}\")\n",
    "        print(f\"  Date: {sample.get('date')}\")\n",
    "        print(f\"  Topic: {sample.get('topic')[:50]}...\" if sample.get('topic') else \"  Topic: N/A\")\n",
    "        print(f\"  Question: {sample.get('question')[:50]}...\" if sample.get('question') else \"  Question: N/A\")\n",
    "        print(f\"  Answer length: {len(sample.get('answer', ''))} characters\")\n",
    "\n",
    "client.close()\n",
    "print(\"\\n✅ Verification completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes"
   },
   "source": [
    "## 📝 หมายเหตุ\n",
    "\n",
    "### เมื่อเสร็จแล้ว:\n",
    "1. ข้อมูลจะถูกบันทึกใน MongoDB Atlas\n",
    "2. ข้อมูลจะไม่ซ้ำกัน (มี unique index บน thread_id)\n",
    "3. สามารถหยุดและเริ่มใหม่ได้ (จะ skip threads ที่มีอยู่แล้ว)\n",
    "\n",
    "### ขั้นตอนต่อไป:\n",
    "- รัน `colab_embedder.ipynb` เพื่อสร้าง embeddings\n",
    "\n",
    "### Tips:\n",
    "- ถ้า Colab timeout ให้รันใหม่ จะเริ่มต่อจากที่ค้างไว้\n",
    "- ถ้าต้องการ scrape เร็วขึ้น ลด delay ใน configuration\n",
    "- ตรวจสอบ logs เพื่อดูความคืบหน้า"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
